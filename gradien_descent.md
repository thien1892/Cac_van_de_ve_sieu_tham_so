# Làm thế nào để tăng tốc độ tính toán gradient descent?  
- Mini- batch gradient
- ADAM (kết hợp gradient descent with momentum và RMSprop)
# Cách giảm suy giảm tỷ lệ học tập?  

1. Mini-batch gradient
